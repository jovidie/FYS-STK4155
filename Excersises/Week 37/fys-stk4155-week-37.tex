\documentclass[english,notitlepage]{revtex4-1}  
% For double-column, add reprint

% Silence warning of revtex4-1
\usepackage{silence}
\WarningFilter{revtex4-1}{Repair the float}

% allows special characters (including æøå)
\usepackage[utf8]{inputenc}
%\usepackage[english]{babel}

\usepackage{physics,amssymb}  % mathematical symbols (physics imports amsmath)
% \usepackage{amsmath}
% \include{amsmath}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{subfigure}
\usepackage{float}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{subfigure}
\usepackage{tikz}
\usetikzlibrary{quantikz}

% Blending two colors:  blue!80!black  =  80% blue and 20% black
\hypersetup{ 
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}}


%% USEFUL LINKS FROM FYS3150:
%%
%%   UiO LaTeX guides:        https://www.mn.uio.no/ifi/tjenester/it/hjelp/latex/
%%   mathematics:             https://en.wikibooks.org/wiki/LaTeX/Mathematics

%%   PHYSICS !                https://mirror.hmc.edu/ctan/macros/latex/contrib/physics/physics.pdf

%%   the basics of Tikz:       https://en.wikibooks.org/wiki/LaTeX/PGF/Tikz
%%   all the colors!:          https://en.wikibooks.org/wiki/LaTeX/Colors
%%   how to draw tables:       https://en.wikibooks.org/wiki/LaTeX/Tables
%%   code listing styles:      https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings
%%   \includegraphics          https://en.wikibooks.org/wiki/LaTeX/Importing_Graphics
%%   learn more about figures  https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions
%%   automagic bibliography:   https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management  (this one is kinda difficult the first time)
%%   REVTeX Guide:             http://www.physics.csbsju.edu/370/papers/Journal_Style_Manuals/auguide4-1.pdf


\begin{document}

\title{FYS-STK4155 Week 37}
\author{Janita Ovidie Sandtrøen Willumsen}
\date{\today}
\noaffiliation


\maketitle 


    
% \textit{List a link to your github repository here!}
    
\section*{Exercise 1}
We have assumed that our data can be described by the continous function 
$f(\boldsymbol{x})$, and an error term $\boldsymbol{\epsilon} ~ N(0, \sigma^{2})$. 
If we approximate the function with the solution derived from a model $\boldsymbol{\tilde{y}} = X\boldsymbol{\beta}$ the data can be described with $\boldsymbol{y} = X\boldsymbol{\beta} + \boldsymbol{\epsilon}$. 
The expectation value 

\begin{align*}
    %\hskip\parindent
    \mathbb{E}(\boldsymbol{y}) &= \mathbb{E}(X\boldsymbol{\beta} + \boldsymbol{\epsilon}) \\
    &= \mathbb{E}(X\boldsymbol{\beta}) + \mathbb{E}(\boldsymbol{\epsilon}) && \text{where the expected value $\boldsymbol{\epsilon} = 0$} \\
    \mathbb{E}(y_{i}) &= \sum_{j=0}^{P-1} X_{i,j} \beta_{j} && \text{for the each element} \\
    &= X_{i,*} \beta_{i} && \text{where $_{*}$ replace the sum over index $i$} \\
\end{align*}


The variance for the element $y_{i}$ can be found by
\begin{align*}
    \mathbb{V}(y_{i}) &= \mathbb{E} \big[ (y_{i} - \mathbb{E}(y_{i}))^{2} \big] \\
    &= \mathbb{E} (y_{i}^{2}) - (\mathbb{E}(y_{i})^{2}) \\
    &= \mathbb{E} ((X_{i,*} \beta_{i} + \epsilon_{i})^{2}) - (X_{i,*} \beta_{i})^{2} \\
    &= \mathbb{E} ((X_{i,*} \beta_{i})^{2} + 2\epsilon_{i}X_{i,*} \beta_{i} + \epsilon^{2}) - (X_{i,*} \beta_{i})^{2} \\
    &= \mathbb{E} ((X_{i,*} \beta_{i})^{2}) + \mathbb{E} (2\epsilon_{i}X_{i,*} \beta_{i}) + \mathbb{E} (\epsilon^{2}) - (X_{i,*} \beta_{i})^{2} \\
    &= (X_{i,*} \beta_{i})^{2} + \mathbb{E} (\epsilon^{2}) - (X_{i,*} \beta_{i})^{2} \\
    &= \mathbb{E} (\epsilon^{2}) = \sigma^{2} \\
\end{align*}

The expression for the optimal parameter 
\begin{align*}
    \boldsymbol{\hat{\beta}} &= (\boldsymbol{X}^{T} \boldsymbol{X})^{-1} \boldsymbol{X}^{T} \boldsymbol{y} \\
\end{align*}
We find the expected value of $\boldsymbol{\hat{\beta}}$
\begin{align*}
    \mathbb{E}(\boldsymbol{\hat{\beta}}) &= \mathbb{E}((\boldsymbol{X}^{T} \boldsymbol{X})^{-1} \boldsymbol{X}^{T} \boldsymbol{y}) \\
    &= (\boldsymbol{X}^{T} \boldsymbol{X})^{-1} \boldsymbol{X}^{T} \mathbb{E}(\boldsymbol{y}) && \text{using that $\boldsymbol{X}$ is a non-stochastic variable} \\
    &= (\boldsymbol{X}^{T} \boldsymbol{X})^{-1} \boldsymbol{X}^{T} \boldsymbol{X} \boldsymbol{\beta} && \text{using $\mathbb{E}(\boldsymbol{y}) = \boldsymbol{X} \boldsymbol{\beta}$} \\
    &= \boldsymbol{\beta} \\
\end{align*}
we can find the variance by 
\begin{align*}
    \mathbb{V}(\boldsymbol{\hat{\beta}}) &= \mathbb{E} \big[ (\boldsymbol{\hat{\beta}} - \mathbb{E}(\boldsymbol{\hat{\beta}}))^{2} \big] \\
    &= \mathbb{E} (\boldsymbol{\hat{\beta}} \boldsymbol{\hat{\beta}}^{T}) - \mathbb{E}(\boldsymbol{\hat{\beta}})^{2}  \\
    &= \mathbb{E} (((\boldsymbol{X}^{T} \boldsymbol{X})^{-1} \boldsymbol{X}^{T} \boldsymbol{y}) ((\boldsymbol{X}^{T} \boldsymbol{X})^{-1} \boldsymbol{X}^{T} \boldsymbol{y})^{T}) - \boldsymbol{\hat{\beta}}\boldsymbol{\hat{\beta}}^{T}  \\
    &= \mathbb{E} ((\boldsymbol{X}^{T} \boldsymbol{X})^{-1} \boldsymbol{X}^{T} \boldsymbol{y} \boldsymbol{y}^{T} \boldsymbol{X} (\boldsymbol{X}^{T} \boldsymbol{X})^{-1}) - \boldsymbol{\hat{\beta}}\boldsymbol{\hat{\beta}}^{T}  \\
    &= (\boldsymbol{X}^{T} \boldsymbol{X})^{-1} \boldsymbol{X}^{T} \mathbb{E} (\boldsymbol{y} \boldsymbol{y}^{T}) \boldsymbol{X} (\boldsymbol{X}^{T} \boldsymbol{X})^{-1} - \boldsymbol{\hat{\beta}}\boldsymbol{\hat{\beta}}^{T}  \\
    &= (\boldsymbol{X}^{T} \boldsymbol{X})^{-1} \boldsymbol{X}^{T} (\boldsymbol{X} \boldsymbol{\beta} \boldsymbol{\beta}^{T} \boldsymbol{X}^{T} + \sigma^{2}) \boldsymbol{X} (\boldsymbol{X}^{T} \boldsymbol{X})^{-1} - \boldsymbol{\hat{\beta}}\boldsymbol{\hat{\beta}}^{T}  \\
    &= \boldsymbol{\beta} \boldsymbol{\beta}^{T} + \sigma^{2}((\boldsymbol{X}^{T} \boldsymbol{X})^{-1} \boldsymbol{X}^{T} \boldsymbol{X} (\boldsymbol{X}^{T} \boldsymbol{X})^{-1}) - \boldsymbol{\hat{\beta}}\boldsymbol{\hat{\beta}}^{T}  \\
    &= \sigma^{2}(\boldsymbol{X}^{T} \boldsymbol{X})^{-1} \\
\end{align*}
Knowing the expectation value and the variance of $\boldsymbol{\hat{\beta}}$, we can define a confidence intervall for each $\hat{\beta}_{j} \pm std(\hat{\beta}_j)$ for $j=1, 2, \hdots, P-1$.

\section*{Exercise 2}
Last week we showed that the optimal $\boldsymbol{\hat{\beta}}^{Ridge}$ can be derived from MSE, and is defined as 
\begin{align*}
    \boldsymbol{\hat{\beta}}^{Ridge} &= (\boldsymbol{X}^{T}\boldsymbol{X} + \lambda \boldsymbol{I})^{-1} \boldsymbol{X}^{T} \boldsymbol{y} \\
\end{align*}
The expectation value is then 
\begin{align*}
    \mathbb{E} (\boldsymbol{\hat{\beta}}^{Ridge}) &= \mathbb{E}((\boldsymbol{X}^{T}\boldsymbol{X} + \lambda \boldsymbol{I})^{-1} \boldsymbol{X}^{T} \boldsymbol{y}) \\
    &= (\boldsymbol{X}^{T}\boldsymbol{X} + \lambda \boldsymbol{I})^{-1} \boldsymbol{X}^{T} \mathbb{E}( \boldsymbol{y} ) && \text{since $\boldsymbol{X}$ and $\lambda \boldsymbol{I}$ are non-stochastic variables} \\
    &= (\boldsymbol{X}^{T}\boldsymbol{X} + \lambda \boldsymbol{I})^{-1} \boldsymbol{X}^{T} \boldsymbol{X} \boldsymbol{\beta} && \text{using $\mathbb{E} (\boldsymbol{y})$ from exercise 1} \\
\end{align*}
For $\lambda = 0$ we have $\mathbb{E} (\boldsymbol{\hat{\beta}}^{OLS})$. The variance 
\begin{align*}
    \mathbb{V}(\boldsymbol{\hat{\beta}}^{Ridge}) &= \mathbb{E} (\boldsymbol{\hat{\beta}}_{R} \boldsymbol{\hat{\beta}}_{R}^{T}) - (\mathbb{E} (\boldsymbol{\hat{\beta}}_{R}))^{2} \\
    &= \mathbb{E} (((\boldsymbol{X}^{T}\boldsymbol{X} + \lambda \boldsymbol{I})^{-1} \boldsymbol{X}^{T} \boldsymbol{y}) (((\boldsymbol{X}^{T}\boldsymbol{X} + \lambda \boldsymbol{I})^{-1} \boldsymbol{X}^{T} \boldsymbol{y}))^{T}) - (\mathbb{E} (\boldsymbol{\hat{\beta}}_{R}))^{2} \\
    &= \mathbb{E} ((\boldsymbol{X}^{T}\boldsymbol{X} + \lambda \boldsymbol{I})^{-1} \boldsymbol{X}^{T} \boldsymbol{y} \boldsymbol{y}^{T} \boldsymbol{X} ((\boldsymbol{X}^{T}\boldsymbol{X} + \lambda \boldsymbol{I})^{-1})^{T} ) - (\mathbb{E} (\boldsymbol{\hat{\beta}}_{R}))^{2} \\
    &= (\boldsymbol{X}^{T}\boldsymbol{X} + \lambda \boldsymbol{I})^{-1} \boldsymbol{X}^{T} \mathbb{E} (\boldsymbol{y} \boldsymbol{y}^{T} ) \boldsymbol{X} ((\boldsymbol{X}^{T}\boldsymbol{X} + \lambda \boldsymbol{I})^{-1})^{T} - (\mathbb{E} (\boldsymbol{\hat{\beta}}_{R}))^{2} \\
    &= (\boldsymbol{X}^{T}\boldsymbol{X} + \lambda \boldsymbol{I})^{-1} \boldsymbol{X}^{T} (\boldsymbol{X} \boldsymbol{\beta} \boldsymbol{\beta}^{T} \boldsymbol{X}^{T} + \sigma^{2}) \boldsymbol{X} ((\boldsymbol{X}^{T}\boldsymbol{X} + \lambda \boldsymbol{I})^{-1})^{T} - (\mathbb{E} (\boldsymbol{\hat{\beta}}_{R}))^{2} \\
    &= \sigma^{2}(\boldsymbol{X}^{T}\boldsymbol{X} + \lambda \boldsymbol{I})^{-1} \boldsymbol{X}^{T} \boldsymbol{X} ((\boldsymbol{X}^{T}\boldsymbol{X} + \lambda \boldsymbol{I})^{-1})^{T} + (\mathbb{E} (\boldsymbol{\hat{\beta}}_{R}))^{2} - (\mathbb{E} (\boldsymbol{\hat{\beta}}_{R}))^{2} \\
    &= \sigma^{2}(\boldsymbol{X}^{T}\boldsymbol{X} + \lambda \boldsymbol{I})^{-1} \boldsymbol{X}^{T} \boldsymbol{X} ((\boldsymbol{X}^{T}\boldsymbol{X} + \lambda \boldsymbol{I})^{-1})^{T} \\
\end{align*}


\end{document}