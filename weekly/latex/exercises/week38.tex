%==========================================================
\section{Exercise 1}\label{sec:ex1}
%==========================================================
The parameters $\mathbf{\beta}$ can be found by optimizing the cost function
\begin{equation*}
    C(\mathbf{X}, \mathbf{\beta}) = \frac{1}{n} \sum_{i=0}^{n-1} ( y_{i} - \Tilde{y}_{i} )^{2} = \mathbb{E} \big[ ( \mathbf{y} - \mathbf{\Tilde{y}} )^{2} \big] .
\end{equation*}

Assuming linearity, the expression can be written as 
\begin{equation*}
    \mathbb{E} \big[ ( \mathbf{y} - \mathbf{\Tilde{y}} )^{2} \big] = \mathbb{E} [ \mathbf{y}^{2} ] - 2 \mathbb{E} [ \mathbf{y} \mathbf{\Tilde{y}} ] + \mathbb{E} [ \mathbf{\Tilde{y}}^{2} ].
\end{equation*}

Looking at the first term
\begin{align*}
    \mathbb{E} [ \mathbf{y}^{2} ] &= \mathbb{E} [ ( f(\mathbf{x}) + \mathbf{\epsilon} )^{2} ] \\
    &= \mathbb{E} [ f(\mathbf{x})^{2} ] - 2 \mathbb{E} [ f(\mathbf{x}) \mathbf{\epsilon} ] + \mathbb{E} [ \mathbf{\epsilon}^{2} ] & \text{where $f(\mathbf{x})$ and $\epsilon$ are independent of $\mathbf{y}$, and each other} \\
    &= f(\mathbf{x})^{2} + \sigma^{2} .
\end{align*}

The second term can be written as 
\begin{align*}
    \mathbb{E} [ \mathbf{y} \mathbf{\Tilde{y}} ] &= \mathbb{E} [ f(\mathbf{x} + \mathbf{\epsilon}) \mathbf{\Tilde{y}} ] \\
    &= \mathbb{E} [ f(\mathbf{x}) \mathbf{\Tilde{y}} ] + \mathbb{E} [ \epsilon \mathbf{\Tilde{y}} ] \\
    &=  f(\mathbf{x}) \mathbb{E} [ \mathbf{\Tilde{y}} ] & \text{since $\mathbb{E}[\mathbf{\epsilon}] = 0$} .
\end{align*}

The last term is the 2. moment, which can be written as 
\begin{align*}
    \mathbb{E} [ \mathbf{\Tilde{y}}^{2} ] &= \mathbb{V}[\mathbf{\Tilde{y}}] + (\mathbb{E} [ \mathbf{\Tilde{y}} ])^{2} 
\end{align*}

Combining all the terms, and rearranging gives us
\begin{align*}
    \mathbb{E} [ \mathbf{y}^{2} ] - 2 \mathbb{E} [ \mathbf{y} \mathbf{\Tilde{y}} ] + \mathbb{E} [ \mathbf{\Tilde{y}}^{2} ] &= f(\mathbf{x})^{2} + \sigma^{2} - 2 f(\mathbf{x}) \mathbb{E} [ \mathbf{\Tilde{y}} ] + \mathbb{V}[\mathbf{\Tilde{y}}] + (\mathbb{E} [ \mathbf{\Tilde{y}} ])^{2} 
    &= \mathbb{E} [ (\mathbf{y} - \mathbf{\Tilde{y}})^{2} ] + \mathbb{V}[\mathbf{\Tilde{y}}] + \sigma^{2} , 
\end{align*}
where 
\begin{align*}
    \mathbb{E} [ (\mathbf{y} - \mathbf{\Tilde{y}})^{2} ] = \text{Bias}[\mathbf{\Tilde{y}}]. 
\end{align*}

The bias-variance trade-off is a way to evaluate the how well the model is able to fit the test data. As seen in Figure \ref{fig:bias-variance}, when the polynomial degree is low, there is not much variance between the training data and the test data. However, the bias might be higher as it fails to predict the right complexity of the data. As the polynomial degree of the input increases, the variance increases. The bias decreases until the sufficient model complexity is reached. This is where the model is able to fit the training data, without overfitting, and also make good predictions on test data. 

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{latex/figures/ex38_noise.pdf}
    \caption{Bias-variance trade-off}
    \label{fig:bias-variance}
\end{figure}

\newpage

\begin{lstlisting}[language=Python]
import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline

# Imports from own code
from utils import design_matrix
from models import LinearReg


def exercise_38_1():
    n = 40
    degree = 15
    p_degrees = np.arange(degree)
    n_boostraps = 100

    x = np.linspace(-3, 3, n).reshape(-1, 1)
    noise = np.random.normal(0, 0.1, x.shape)
    y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2) + noise 

    error_train = np.empty(degree)
    error_test = np.zeros(degree)
    bias = np.zeros(degree)
    variance = np.zeros(degree)

    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

    for p in p_degrees:
        model = make_pipeline(PolynomialFeatures(degree=p), LinearRegression(fit_intercept=False))
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test).ravel()

        error_test[p] = np.mean( np.mean((y_test - y_pred)**2))
        bias[p] = np.mean( (y_test - np.mean(y_pred))**2)
        variance[p] = np.mean(np.var(y_pred))

    return (p_degrees, error_test, bias, variance)

def exercise_38_2():
    n = 40
    degree = 12
    p_degrees = np.arange(degree)
    n_bootstraps = 100

    x = np.random.rand(100).reshape(-1, 1)
    noise = np.random.normal(0, 0.1, x.shape)
    y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2) + noise 

    # error_train = np.empty(degree)
    error_test = np.empty(degree)
    bias = np.empty(degree)
    variance = np.empty(degree)
    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

    for p in range(degree):
        Xtrain = design_matrix(X_train, p)
        Xtest = design_matrix(X_test, p)
        
        y_tilde = np.empty((y_train.shape[0], n_bootstraps))
        y_pred = np.empty((y_test.shape[0], n_bootstraps))

        # mse_train = np.empty(n_bootstraps)
        mse_test = np.empty(n_bootstraps)

        for i in range(n_bootstraps):
            X_, y_ = resample(Xtrain, y_train)
            model = LinearReg()
            model.fit(X_, y_)

            y_tilde[:, i] = model.predict(X_).ravel()
            y_pred[:, i] = model.predict(X_test).ravel()

            # mse_train[i] = mse(y_, y_tilde[:, i])
            mse_test[i] = mse(y_test, y_pred[:, i])

        # error_train[p] = np.mean(mse_train)
        error_test[p] = np.mean(mse_test)
        bias[p] = np.mean((y_test - np.mean(y_pred, axis=1))**2)
        variance[p] = np.mean(np.var(y_pred, axis=1, keepdims=True))

    # return (p_degrees, error_train, error_test, bias, variance)
    return (p_degrees, error_test, bias, variance)

if __name__ == '__main__':
    np.random.seed(2024)
    p_degrees, error_test, bias, variance = exercise_38_1()

    # Need fixing to work
    # p_degrees, error_test, bias, variance = exercise_38_2()

    fig, ax = plt.subplots()
    ax.plot(p_degrees, error_test, label=r"$\epsilon_{test}$")
    ax.plot(p_degrees, bias, label="Bias")
    ax.plot(p_degrees, variance, label="Variance")
    ax.legend()
    # plt.show()
    fig.savefig("../latex/figures/ex38_noise.pdf")
\end{lstlisting}
